{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNFL_Q1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1knRwdnNvqCj"
      },
      "source": [
        "# BITS F312 - Neural Network and Fuzzy Logic\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kL1ugGKAEgK"
      },
      "source": [
        "# NNFL Assignment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omj7b7rT_iRC",
        "outputId": "84d8f314-031a-4abc-afc0-6bdc11708e8d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESmgBJdcAAz5",
        "outputId": "ea25fe4a-d619-4780-e459-246ffa54923a"
      },
      "source": [
        "# Changing directory to the directory containing dataset\n",
        "%cd drive/MyDrive/NNFL/Data_A2/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NNFL/Data_A2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meFTsfIpAeGP",
        "outputId": "1fb8a342-2162-4262-f703-1f2d6d6a2c49"
      },
      "source": [
        "# listing datasets\n",
        "%ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1234719\n",
            "-rw------- 1 root root     637638 Oct 31 04:55 Assignment2.pdf\n",
            "-rw------- 1 root root        259 Oct 31 04:57 class_label.mat\n",
            "-rw------- 1 root root      40295 Oct 31 04:57 data55.xlsx\n",
            "-rw------- 1 root root      21269 Oct 31 04:55 data5.xlsx\n",
            "-rw------- 1 root root 1263647365 Oct 31 04:58 input.mat\n",
            "drwx------ 2 root root       4096 Nov 18 05:58 \u001b[0m\u001b[01;34mlogs\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monX5050Awmc"
      },
      "source": [
        "# libraries required\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wDLjGQSaEU"
      },
      "source": [
        "# supressing warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cubp0wJ0Bjyz"
      },
      "source": [
        "# Q1 \n",
        "Implement non-linear perceptron algorithm for the classification using Online Learning (Hebbian learning)\n",
        "algorithm. The dataset (data55.xlsx) contains 19 features and the last column is the output (class label).\n",
        "You can use hold-out cross-validation (70, 10, and 20%) for the selection of training, validation and test\n",
        "instances. Evaluate accuracy, sensitivity and specificity measures for the evaluation of test instances\n",
        "(Packages such as Scikitlearn, keras, tensorflow, pytorch etc. are not allowed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqJTKvnxC2Rg"
      },
      "source": [
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  Returns (float) sigmoid of the input variable x\n",
        "  \"\"\"\n",
        "  val = 1/(1+ np.exp(-x))\n",
        "  return val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHRw19JTDUUt"
      },
      "source": [
        "def sigmoidDerivative(x):\n",
        "  \"\"\"\n",
        "  Returns (float) the derivative of the sigmoid of the input variable x\n",
        "  \"\"\"\n",
        "  val =  x * (1 - x)\n",
        "  return val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n9z9WgODcPk"
      },
      "source": [
        "def perceptron(X_train_data, Y_train_data, bias, W, alpha = 0.001, epochs = 20000):\n",
        "  \n",
        "  for i in range(epochs):\n",
        "\n",
        "    layer = np.dot(X_train_data, W)\n",
        "    input = layer + bias\n",
        "    output = sigmoid(input)\n",
        "\n",
        "    error = output - Y_train_data\n",
        "    derivative = sigmoidDerivative(output)\n",
        "    update = error*derivative\n",
        "    WNew = np.dot(X_train_data.T, update)\n",
        "    W = W - alpha*WNew\n",
        "    update_bias = update\n",
        "    bias = bias - alpha*update\n",
        "\n",
        "  return W, bias\n",
        "def pred_eval(X, W, bias):\n",
        "  layer = np.dot(X, W)\n",
        "  input = layer + bias[0]\n",
        "  output = sigmoid(input)\n",
        "\n",
        "  return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvIBYe9qFKpx"
      },
      "source": [
        "def resultQ1(filename = 'data55.xlsx'):\n",
        "  dataset = pd.read_excel(filename, header = None)\n",
        "\n",
        "  row, col = dataset.shape\n",
        "  feats = col - 1 \n",
        "\n",
        "  # normalization\n",
        "  dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "  \n",
        "  # spliting dataset into train test and val\n",
        "  training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "  training_data = np.array(training_data)\n",
        "  validation_data = np.array(validation_data)\n",
        "  testing_data = np.array(testing_data)\n",
        "  training_data_X = training_data[:, :feats]\n",
        "  training_data_y = training_data[:, feats]\n",
        "  validation_data_X = validation_data[:, :feats]\n",
        "  validation_data_y = validation_data[:, feats]\n",
        "  testing_data_X = testing_data[:, :feats]\n",
        "  testing_data_y = testing_data[:, feats]\n",
        "\n",
        "  train_row, train_col = training_data_X.shape\n",
        "\n",
        "  W = np.random.randn(train_col)  \n",
        "  bias = np.ones(train_row)\n",
        "\n",
        "  W, bias = perceptron(training_data_X, training_data_y, bias, W)\n",
        "  print(\"The Weights after training is as follows: \\n\")\n",
        "  pprint(W)\n",
        "  print(\"The Bias after training is as follows: \", bias[0])\n",
        "\n",
        "  train_pred = pred_eval(training_data_X, W, bias)\n",
        "  train_pred = np.where(train_pred > 0.475, 1,0)\n",
        "  print(\"Training Accuracy: \", (np.abs(np.sum(train_pred == training_data_y))/len(training_data_y)))\n",
        "\n",
        "  test_pred = pred_eval(testing_data_X, W, bias)\n",
        "  test_pred = np.where(test_pred > 0.475, 1,0)\n",
        "  print(\"Testing Accuracy: \", (np.abs(np.sum(test_pred == testing_data_y))/len(testing_data_y)))\n",
        "  \n",
        "  validation_pred = pred_eval(validation_data_X, W, bias)\n",
        "  validation_pred = np.where(validation_pred > 0.475, 1,0)\n",
        "  print(\"Validation Accuracy: \", (np.abs(np.sum(validation_pred == validation_data_y))/len(validation_data_y)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVNAUdGEPonq"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAs9yjc0HZ8N",
        "outputId": "c4685485-6a66-42a1-9894-65a8cb569281"
      },
      "source": [
        "resultQ1()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Weights after training is as follows: \n",
            "\n",
            "array([ 0.41205131, -0.33590672, -0.43932103,  1.56798797,  0.21070115,\n",
            "       -0.13992397, -0.13812113,  0.03820567,  1.06120044,  0.15026506,\n",
            "        0.9207862 ,  0.73246924, -0.02641763,  0.62917204, -1.44522532,\n",
            "        0.45782587,  0.02331333, -1.22551068,  1.00836779])\n",
            "The Bias after training is as follows:  1.3100516253516938\n",
            "Training Accuracy:  0.9378549098309535\n",
            "Testing Accuracy:  0.8647588581456906\n",
            "Validation Accuracy:  0.9673190278858435\n"
          ]
        }
      ]
    }
  ]
}