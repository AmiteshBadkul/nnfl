{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNFL_A2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kL1ugGKAEgK"
      },
      "source": [
        "# NNFL Assignment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omj7b7rT_iRC",
        "outputId": "84d8f314-031a-4abc-afc0-6bdc11708e8d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESmgBJdcAAz5",
        "outputId": "ea25fe4a-d619-4780-e459-246ffa54923a"
      },
      "source": [
        "# Changing directory to the directory containing dataset\n",
        "%cd drive/MyDrive/NNFL/Data_A2/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NNFL/Data_A2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meFTsfIpAeGP",
        "outputId": "1fb8a342-2162-4262-f703-1f2d6d6a2c49"
      },
      "source": [
        "# listing datasets\n",
        "%ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1234719\n",
            "-rw------- 1 root root     637638 Oct 31 04:55 Assignment2.pdf\n",
            "-rw------- 1 root root        259 Oct 31 04:57 class_label.mat\n",
            "-rw------- 1 root root      40295 Oct 31 04:57 data55.xlsx\n",
            "-rw------- 1 root root      21269 Oct 31 04:55 data5.xlsx\n",
            "-rw------- 1 root root 1263647365 Oct 31 04:58 input.mat\n",
            "drwx------ 2 root root       4096 Nov 18 05:58 \u001b[0m\u001b[01;34mlogs\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monX5050Awmc"
      },
      "source": [
        "# libraries required\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wDLjGQSaEU"
      },
      "source": [
        "# supressing warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cubp0wJ0Bjyz"
      },
      "source": [
        "# Q1 \n",
        "Implement non-linear perceptron algorithm for the classification using Online Learning (Hebbian learning)\n",
        "algorithm. The dataset (data55.xlsx) contains 19 features and the last column is the output (class label).\n",
        "You can use hold-out cross-validation (70, 10, and 20%) for the selection of training, validation and test\n",
        "instances. Evaluate accuracy, sensitivity and specificity measures for the evaluation of test instances\n",
        "(Packages such as Scikitlearn, keras, tensorflow, pytorch etc. are not allowed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqJTKvnxC2Rg"
      },
      "source": [
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  Returns (float) sigmoid of the input variable x\n",
        "  \"\"\"\n",
        "  val = 1/(1+ np.exp(-x))\n",
        "  return val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHRw19JTDUUt"
      },
      "source": [
        "def sigmoidDerivative(x):\n",
        "  \"\"\"\n",
        "  Returns (float) the derivative of the sigmoid of the input variable x\n",
        "  \"\"\"\n",
        "  val =  x * (1 - x)\n",
        "  return val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n9z9WgODcPk"
      },
      "source": [
        "def perceptron(X_train_data, Y_train_data, bias, W, alpha = 0.001, epochs = 20000):\n",
        "  \n",
        "  for i in range(epochs):\n",
        "\n",
        "    layer = np.dot(X_train_data, W)\n",
        "    input = layer + bias\n",
        "    output = sigmoid(input)\n",
        "\n",
        "    error = output - Y_train_data\n",
        "    derivative = sigmoidDerivative(output)\n",
        "    update = error*derivative\n",
        "    WNew = np.dot(X_train_data.T, update)\n",
        "    W = W - alpha*WNew\n",
        "    update_bias = update\n",
        "    bias = bias - alpha*update\n",
        "\n",
        "  return W, bias\n",
        "def pred_eval(X, W, bias):\n",
        "  layer = np.dot(X, W)\n",
        "  input = layer + bias[0]\n",
        "  output = sigmoid(input)\n",
        "\n",
        "  return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvIBYe9qFKpx"
      },
      "source": [
        "def resultQ1(filename = 'data55.xlsx'):\n",
        "  dataset = pd.read_excel(filename, header = None)\n",
        "\n",
        "  row, col = dataset.shape\n",
        "  feats = col - 1 \n",
        "\n",
        "  # normalization\n",
        "  dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "  \n",
        "  # spliting dataset into train test and val\n",
        "  training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "  training_data = np.array(training_data)\n",
        "  validation_data = np.array(validation_data)\n",
        "  testing_data = np.array(testing_data)\n",
        "  training_data_X = training_data[:, :feats]\n",
        "  training_data_y = training_data[:, feats]\n",
        "  validation_data_X = validation_data[:, :feats]\n",
        "  validation_data_y = validation_data[:, feats]\n",
        "  testing_data_X = testing_data[:, :feats]\n",
        "  testing_data_y = testing_data[:, feats]\n",
        "\n",
        "  train_row, train_col = training_data_X.shape\n",
        "\n",
        "  W = np.random.randn(train_col)  \n",
        "  bias = np.ones(train_row)\n",
        "\n",
        "  W, bias = perceptron(training_data_X, training_data_y, bias, W)\n",
        "  print(\"The Weights after training is as follows: \\n\")\n",
        "  pprint(W)\n",
        "  print(\"The Bias after training is as follows: \", bias[0])\n",
        "\n",
        "  train_pred = pred_eval(training_data_X, W, bias)\n",
        "  train_pred = np.where(train_pred > 0.475, 1,0)\n",
        "  print(\"Training Accuracy: \", (np.abs(np.sum(train_pred == training_data_y))/len(training_data_y)))\n",
        "\n",
        "  test_pred = pred_eval(testing_data_X, W, bias)\n",
        "  test_pred = np.where(test_pred > 0.475, 1,0)\n",
        "  print(\"Testing Accuracy: \", (np.abs(np.sum(test_pred == testing_data_y))/len(testing_data_y)))\n",
        "  \n",
        "  validation_pred = pred_eval(validation_data_X, W, bias)\n",
        "  validation_pred = np.where(validation_pred > 0.475, 1,0)\n",
        "  print(\"Validation Accuracy: \", (np.abs(np.sum(validation_pred == validation_data_y))/len(validation_data_y)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVNAUdGEPonq"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAs9yjc0HZ8N",
        "outputId": "c4685485-6a66-42a1-9894-65a8cb569281"
      },
      "source": [
        "resultQ1()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Weights after training is as follows: \n",
            "\n",
            "array([ 0.41205131, -0.33590672, -0.43932103,  1.56798797,  0.21070115,\n",
            "       -0.13992397, -0.13812113,  0.03820567,  1.06120044,  0.15026506,\n",
            "        0.9207862 ,  0.73246924, -0.02641763,  0.62917204, -1.44522532,\n",
            "        0.45782587,  0.02331333, -1.22551068,  1.00836779])\n",
            "The Bias after training is as follows:  1.3100516253516938\n",
            "Training Accuracy:  0.9378549098309535\n",
            "Testing Accuracy:  0.8647588581456906\n",
            "Validation Accuracy:  0.9673190278858435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HCYO5XXPqdq"
      },
      "source": [
        "# Q2 \n",
        "Implement kernel perceptron algorithm for the classification task. The dataset (data55.xlsx) contains 19\n",
        "features and the last column is the output (class label). You can use hold-out cross-validation (70, 10, and\n",
        "20%) for the selection of training, validation and test instances. Evaluate accuracy, sensitivity and\n",
        "specificity measures for the evaluation of test instances. Evaluate the classification performance\n",
        "separately using linear, RBF, and polynomial kernels (Packages such as Scikitlearn, keras, tensorflow,\n",
        "pytorch etc. are not allowed).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5OwTK6D2vOt"
      },
      "source": [
        "def metrics(Y_true, Y_pred):\n",
        "    FP=0 # For counting the False Positives\n",
        "    FN=0 # For counting the False Negatives\n",
        "    TN=0 # For counting the True Negatives\n",
        "    TP=0 # For counting the True Positives\n",
        "\n",
        "    for i in range(len(Y_true)):\n",
        "      if Y_true[i]==1:\n",
        "        if Y_pred[i]==1:\n",
        "          TP+=1\n",
        "        else:\n",
        "          FN+=1\n",
        "      else:\n",
        "        if Y_pred[i]==0:\n",
        "          TN+=1\n",
        "        else:\n",
        "          FP+=1\n",
        "\n",
        "    print('{}'.format('-'*75))\n",
        "    \n",
        "    sens= TP/(TP+FN)\n",
        "    spes = TN/(TN+FP)\n",
        "\n",
        "    print(\"Sensitivity : \", sens)\n",
        "    print(\"Specificity : \", spes)\n",
        "    print(\"Accuracy ((TN+TP)/(TN+TP+FN+FP)) : \", ((TP+TN)/(TN+FN+TP+FP)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hl6xYzCPwey"
      },
      "source": [
        "def polynomialKernel(idx_list, idx, lower, upper, deg):\n",
        "    idx = [*idx]\n",
        "    if deg == 0:\n",
        "        idx_list.append(idx)\n",
        "        return\n",
        "    for i in range(lower, upper):\n",
        "        idx.append(i)\n",
        "        polynomialKernel(idx_list, idx, i, upper, deg-1)\n",
        "        idx = idx[0:-1]\n",
        "\n",
        "def polynomialFeats(X, deg):\n",
        "    row, col = X.shape\n",
        "    feats = []\n",
        "    for i in range(1, deg+1):\n",
        "        l = []\n",
        "        polynomialKernel(l, [], 0, col, i)\n",
        "        for idx in l:\n",
        "            x_temp = np.ones((row,))\n",
        "            for idx in idx:\n",
        "                x_temp = x_temp * X[:, idx]\n",
        "            feats.append(x_temp)\n",
        "    return np.stack(feats, axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECasKjbEQfjH"
      },
      "source": [
        "def resultQ2(filename = 'data55.xlsx'):\n",
        "  dataset = pd.read_excel(filename, header = None)\n",
        "\n",
        "  row, col = dataset.shape\n",
        "  feats = col - 1 \n",
        "\n",
        "  # normalization\n",
        "  dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "  \n",
        "  # spliting dataset into train test and val\n",
        "  training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "  training_data = np.array(training_data)\n",
        "  validation_data = np.array(validation_data)\n",
        "  testing_data = np.array(testing_data)\n",
        "  training_data_X = polynomialFeats(training_data[:, :feats], 4)\n",
        "  training_data_y = training_data[:, feats]\n",
        "  validation_data_X = polynomialFeats(validation_data[:, :feats], 4)\n",
        "  validation_data_y = validation_data[:, feats]\n",
        "  testing_data_X = polynomialFeats(testing_data[:, :feats], 4)\n",
        "  testing_data_y = testing_data[:, feats]\n",
        "\n",
        "  train_row, train_col = training_data_X.shape\n",
        "\n",
        "  W = np.random.randn(train_col)  \n",
        "  bias = np.ones(train_row)\n",
        "\n",
        "  W, bias = perceptron(training_data_X, training_data_y, bias, W)\n",
        "  print(\"The Weights after training is as follows: \\n\")\n",
        "  pprint(W)\n",
        "  print(\"The Bias after training is as follows: \", bias[0])\n",
        "  print('{}'.format('-'*75))\n",
        "\n",
        "  train_pred = pred_eval(training_data_X, W, bias)\n",
        "  train_pred = np.where(train_pred > 0.5, 1,0)\n",
        "  print(\"Training Accuracy: \\n\")\n",
        "  metrics(train_pred, training_data_y)\n",
        "  print('{}'.format('-'*75))\n",
        "\n",
        "  test_pred = pred_eval(testing_data_X, W, bias)\n",
        "  test_pred = np.where(test_pred > 0.5, 1,0)\n",
        "  print(\"Testing Accuracy: \\n\")\n",
        "  metrics(test_pred, testing_data_y)\n",
        "  print('{}'.format('-'*75))\n",
        "  \n",
        "  validation_pred = pred_eval(validation_data_X, W, bias)\n",
        "  validation_pred = np.where(validation_pred > 0.5, 1,0)\n",
        "  print(\"Validation Accuracy: \\n\")\n",
        "  metrics(validation_pred, validation_data_y)\n",
        "  print('{}'.format('-'*75))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqbRKsRo8yUA",
        "outputId": "a6bf4dfa-e8c9-49b1-8fbb-d791614a0066"
      },
      "source": [
        "resultQ2()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Weights after training is as follows: \n",
            "\n",
            "array([ 0.13049155, -1.31189354, -0.67991826, ...,  0.57346655,\n",
            "       -0.80864538,  0.11325281])\n",
            "The Bias after training is as follows:  1.0155357007732724\n",
            "---------------------------------------------------------------------------\n",
            "Training Accuracy: \n",
            "\n",
            "Sensitivity :  0.9456521739130435\n",
            "Specificity :  0.8906256289423758\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.9230769230769231\n",
            "---------------------------------------------------------------------------\n",
            "Testing Accuracy: \n",
            "\n",
            "Sensitivity :  0.8765432098765432\n",
            "Specificity :  0.9523809523809523\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.9254385964912281\n",
            "---------------------------------------------------------------------------\n",
            "Validation Accuracy: \n",
            "\n",
            "Sensitivity :  0.8833333333333333\n",
            "Specificity :  0.8414634146341463\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8591549295774648\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_kUeuFNh9LV"
      },
      "source": [
        "# Q3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMBG928JS-Do"
      },
      "source": [
        "The dataset (data5.xlsx) contains 7 features and the last column is the output (class labels). Design a\n",
        "multilayer perceptron based neural network (two hidden layers) for the classification. You can use both\n",
        "holdout (70, 10, and 20%) and 5-fold cross-validation approaches for evaluating the performance of the\n",
        "classifier (individual accuracy and overall accuracy). You can select the number of hidden neurons of each\n",
        "hidden layer and other MLP parameters using grid-search method. (Packages such as Scikitlearn, keras,\n",
        "tensorflow, pytorch etc. are not allowed). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8-qxX3Kh7Vu"
      },
      "source": [
        "def relu(x):\n",
        "  \n",
        "  return np.maximum(x,0)\n",
        "\n",
        "def reluDerivative(x):\n",
        "  \n",
        "    return np.heaviside(x,1)\n",
        "\n",
        "def sigmoid(x):\n",
        "  \n",
        "  return 1/(1+ np.exp(-x))\n",
        "\n",
        "def sigmoidDerivative(x):\n",
        "  \n",
        "    return x * (1 - x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LZRJaXDlJb6"
      },
      "source": [
        "def perceptronTraining(train_X, train_y, neuron_L1 = 32, neuron_L2 = 16, epochs = 5000, alpha = 0.00125):\n",
        "  neuron_in = 147\n",
        "  neuron_out = 1\n",
        "  weight_L1 = np.random.uniform(size=(neuron_in,neuron_L1))\n",
        "  bias_L1 = np.random.uniform(size=(1,neuron_L1))\n",
        "  weight_L2 = np.random.uniform(size=(neuron_L1,neuron_L2))\n",
        "  bias_L2 = np.random.uniform(size=(1,neuron_L2))\n",
        "  final_weight = np.random.uniform(size=(neuron_L2,neuron_out))\n",
        "  final_bias = np.random.uniform(size=(1,neuron_out))\n",
        "  costs_perceptron = []\n",
        "  for i in range(epochs):\n",
        "\n",
        "\n",
        "    hiddenInpL1 = np.dot(train_X,weight_L1)\n",
        "    hiddenInpL1=hiddenInpL1 + bias_L1\n",
        "    hiddenActL1 = relu(hiddenInpL1)\n",
        "    hiddenInpL2 = np.dot(hiddenActL1,weight_L2)\n",
        "    hiddenInpL2 = hiddenInpL2 + bias_L2\n",
        "    hiddenActL2 = relu(hiddenInpL2)\n",
        "    outInpL = np.dot(hiddenActL2,final_weight)\n",
        "    outInpL = outInpL+ final_bias\n",
        "    output = sigmoid(outInpL)\n",
        "\n",
        "    error_track = output-train_y\n",
        "    slopeOutL = sigmoidDerivative(output)\n",
        "    slopeHidL2 = reluDerivative(hiddenActL2)\n",
        "    slopeHidL1 = reluDerivative(hiddenActL1)\n",
        "    output_derivative = error_track * slopeOutL\n",
        "    errorHidL2 = output_derivative.dot(final_weight.T)\n",
        "    d_hidden2 = errorHidL2*slopeHidL2 \n",
        "    errorHidL1 = d_hidden2.dot(weight_L2.T)\n",
        "    d_hidden1 = errorHidL1*slopeHidL1\n",
        "    final_weight = final_weight- hiddenActL2.T.dot(output_derivative) *alpha\n",
        "    final_bias = final_bias- np.sum(output_derivative, axis=0,keepdims=True) *alpha\n",
        "    weight_L2 = weight_L2- hiddenActL1.T.dot(d_hidden2) *alpha\n",
        "    bias_L2 = bias_L2- np.sum(d_hidden2, axis=0,keepdims=True) *alpha\n",
        "    weight_L1 = weight_L1- train_X.T.dot(d_hidden1) *alpha\n",
        "    bias_L1 = bias_L1- np.sum(d_hidden1, axis=0,keepdims=True) *alpha\n",
        "\n",
        "    error_track = np.asarray(error_track)\n",
        "    costs_perceptron.append(np.sum(0.5*(error_track*error_track)))\n",
        "\n",
        "  return weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias, costs_perceptron\n",
        "def pred_eval(X, weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias):\n",
        "\n",
        "    hiddenInpL1 = np.dot(X, weight_L1)\n",
        "    hiddenInpL1 = hiddenInpL1 + bias_L1\n",
        "    hiddenActL1 =  relu(hiddenInpL1)\n",
        "    hiddenInpL2 = np.dot(hiddenActL1,weight_L2)\n",
        "    hiddenInpL2 = hiddenInpL2 + bias_L2\n",
        "    hiddenActL2 = relu(hiddenInpL2)\n",
        "    outInpL = np.dot(hiddenActL2,final_weight)\n",
        "    outInpL = outInpL+ final_bias\n",
        "    output = sigmoid(outInpL)\n",
        "\n",
        "    return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmQNDTaDoYiy"
      },
      "source": [
        "def resultQ3(filename = 'data5.xlsx'):\n",
        "  dataset = pd.read_excel(filename, header = None)\n",
        "\n",
        "  row, col = dataset.shape\n",
        "  feats = col - 1 \n",
        "\n",
        "  # normalization\n",
        "  dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "  \n",
        "  # spliting dataset into train test and val\n",
        "  training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "  training_data = np.array(training_data)\n",
        "  validation_data = np.array(validation_data)\n",
        "  testing_data = np.array(testing_data)\n",
        "  training_data_X = training_data[:, :feats]\n",
        "  training_data_y = training_data[:, feats]\n",
        "  validation_data_X = validation_data[:, :feats]\n",
        "  validation_data_y = validation_data[:, feats]\n",
        "  testing_data_X = testing_data[:, :feats]\n",
        "  testing_data_y = testing_data[:, feats]\n",
        "\n",
        "  train_row, train_col = training_data_X.shape\n",
        "\n",
        "  weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias, costs_perceptron = perceptronTraining(training_data_X, training_data_y)\n",
        "  temp_cost = costs_perceptron[-1]\n",
        "\n",
        "  train_pred = pred_eval(training_data_X , weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias)\n",
        "  train_pred = np.where(train_pred > 0.5, 1,0)\n",
        "  print(\"Training Accuracy: \\n\")\n",
        "  metrics(train_pred, training_data_y)\n",
        "  print('{}'.format('-'*75))\n",
        "\n",
        "  test_pred = pred_eval(testing_data_X , weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias)\n",
        "  test_pred = np.where(test_pred > 0.5, 1,0)\n",
        "  print(\"Testing Accuracy: \\n\")\n",
        "  metrics(test_pred, testing_data_y)\n",
        "  print('{}'.format('-'*75))\n",
        "  \n",
        "  validation_pred = pred_eval(validation_data_X , weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias)\n",
        "  validation_pred = np.where(validation_pred > 0.5, 1,0)\n",
        "  print(\"Validation Accuracy: \\n\")\n",
        "  metrics(validation_pred, validation_data_y)\n",
        "  print('{}'.format('-'*75))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91f-QAKioYiz",
        "outputId": "bfbf0e2d-6341-489b-ea50-27f7856d8aeb"
      },
      "source": [
        "resultQ3()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: \n",
            "\n",
            "Sensitivity :  0.8305084745762712\n",
            "Specificity :  0.873015873015873\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8524590163934426\n",
            "---------------------------------------------------------------------------\n",
            "Testing Accuracy: \n",
            "\n",
            "Sensitivity :  0.8706896551724138\n",
            "Specificity :  0.8166666666666667\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8432203389830508\n",
            "---------------------------------------------------------------------------\n",
            "Validation Accuracy: \n",
            "\n",
            "Sensitivity :  0.9285714285714286\n",
            "Specificity :  0.8333333333333334\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8793103448275862\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLoM8rZLV7hL"
      },
      "source": [
        "# Q4\n",
        "Implement the radial basis function neural network (RBFNN) for the classification problem. You can use\n",
        "Gaussian, multiquadric and linear kernel functions for the implementation. You can use both holdout (70,\n",
        "10, and 20%) and 5-fold cross-validation approaches for evaluating the performance of the classifier. The\n",
        "classification performance must be evaluated using individual accuracy and overall accuracy measures.\n",
        "The dataset (data5.xlsx) contains 7 features and the last column is the output (class labels). (Packages\n",
        "such as Scikitlearn, keras, tensorflow, pytorch etc. are not allowed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v19SDl_JvmdZ"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def kmeansClustering(k):\n",
        "    kmeans = KMeans(n_clusters = k).fit(train_x)\n",
        "    return kmeans\n",
        "\n",
        "def kernel_func(x, std, sigma, kernalFunction):\n",
        "    beta = 1/(2*sigma*sigma)\n",
        "    if kernalFunction == \"Gaussian\":\n",
        "        return np.exp(-beta*(np.linalg.norm(x-std))**2)\n",
        "    elif kernalFunction == \"MultiQuadratic\":\n",
        "        return ((np.linalg.norm(x-std))**2+sigma**2)**0.5\n",
        "    elif kernalFunction == \"Linear\":\n",
        "        return np.linalg.norm(x-std)\n",
        "\n",
        "def sigmaComputation(x, labels, std):\n",
        "    c = std.shape[0]\n",
        "    sigma = np.zeros(c)\n",
        "    for i in range(c):\n",
        "        x_temp = x[labels==i]\n",
        "        k = 0\n",
        "        for j in range(x_temp.shape[0]):\n",
        "            k+=np.linalg.norm(x_temp[j]-std[i])\n",
        "        sigma[i] = k/x_temp.shape[0]\n",
        "    return sigma\n",
        "\n",
        "def hypothesisCalc(X, std, sigma, kernalFunction):\n",
        "    c = std.shape[0]\n",
        "    H = np.zeros((X.shape[0],c))\n",
        "    for i in range(H.shape[0]):\n",
        "        for j in range(H.shape[1]):\n",
        "            H[i][j]=kernel_func(X[i],std[j],sigma[j],kernalFunction)\n",
        "    return H\n",
        "\n",
        "def compute(train_x, train_y, test_x, test_y, kernalFunction):\n",
        "    kmeans = kmeansClustering(20)\n",
        "    std = kmeans.cluster_centers_\n",
        "    sigma = sigmaComputation(train_x,kmeans.labels_,std)\n",
        "    H = hypothesisCalc(train_x,std,sigma,kernalFunction)\n",
        "    W = np.dot(np.linalg.pinv(H),train_y)\n",
        "    H = hypothesisCalc(test_x,std,sigma,kernalFunction)\n",
        "    pred = np.dot(H,W)\n",
        "    p = (pred>0.5).astype(int)\n",
        "    a = (p!=test_y).astype(int)\n",
        "    metrics(test_y, pred_y) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv7EI9DGvosZ"
      },
      "source": [
        "def resultQ4(filename = 'data5.xlsx'):\n",
        "  dataset = pd.read_excel(filename, header = None)\n",
        "\n",
        "  row, col = dataset.shape\n",
        "  feats = col - 1 \n",
        "\n",
        "  # normalization\n",
        "  dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "  \n",
        "  # spliting dataset into train test and val\n",
        "  training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "  training_data = np.array(training_data)\n",
        "  validation_data = np.array(validation_data)\n",
        "  testing_data = np.array(testing_data)\n",
        "  training_data_X = training_data[:, :feats]\n",
        "  training_data_y = training_data[:, feats]\n",
        "  validation_data_X = validation_data[:, :feats]\n",
        "  validation_data_y = validation_data[:, feats]\n",
        "  testing_data_X = testing_data[:, :feats]\n",
        "  testing_data_y = testing_data[:, feats]\n",
        "\n",
        "  train_row, train_col = training_data_X.shape\n",
        "\n",
        "  print('Linear Kernel')\n",
        "  compute(training_data_X, training_data_y, testing_data_X, testing_data_y, \"Linear\")\n",
        "  compute(training_data_X, training_data_y, validation_data_X, validation_data_y, \"Linear\")\n",
        "\n",
        "  print('Multi Quadratic Kernel')\n",
        "  compute(training_data_X, training_data_y, testing_data_X, testing_data_y, \"MultiQuadratic\")\n",
        "  compute(training_data_X, training_data_y, validation_data_X, validation_data_y, \"MultiQuadratic\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msDhstLMZCpL",
        "outputId": "99906ff4-f4e5-4701-b21f-3fb137e48d9d"
      },
      "source": [
        "resultQ4()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel\n",
            "Testing Accuracy: \n",
            "\n",
            "Sensitivity :  0.8461538461538461\n",
            "Specificity :  0.7209302325581395\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.7804878048780488\n",
            "---------------------------------------------------------------------------\n",
            "Validation Accuracy: \n",
            "\n",
            "Sensitivity :  0.8620689655172413\n",
            "Specificity :  0.9183673469387755\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8878504672897196\n",
            "---------------------------------------------------------------------------\n",
            "Multi Quadratic Kernel\n",
            "Testing Accuracy: \n",
            "\n",
            "Sensitivity :  0.7868852459016393\n",
            "Specificity :  0.8636363636363636\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8267716535433071\n",
            "---------------------------------------------------------------------------\n",
            "Validation Accuracy: \n",
            "\n",
            "Sensitivity :  0.8837209302325582\n",
            "Specificity :  0.803921568627451\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8404255319148937\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng-UZ-q5YRaH"
      },
      "source": [
        "# Q5 \n",
        "Implement the stacked autoencoder based deep neural network for the classification problem. The deep\n",
        "neural network must contain 3 hidden layers from three autoencoders. You can use holdout (70, 10, and\n",
        "20%) cross-validation technique for selecting, training and test instances for the classifier. The dataset\n",
        "(data5.xlsx) contains 7 features and the last column is the output (class labels). For autoencoder\n",
        "implementation, please use back propagation algorithm discussed in the class. Evaluate individual\n",
        "accuracy and overall accuracy. (Packages such as Scikitlearn, keras, tensorflow, pytorch etc. are not\n",
        "allowed).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mpw-xgdCi2n"
      },
      "source": [
        "\n",
        "def sigmoidFuntion(Z):\n",
        "    return 1/(1+np.exp(-Z)),Z\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    return (Z,A)\n",
        "\n",
        "def reluBack(dA, cache):\n",
        "    Z = cache\n",
        "    \n",
        "    dZ = np.array(dA, copy=True) \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoidBack(dA, cache):\n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "def initializeParams(layerDimensions, para, stack):\n",
        "    parameters = {}\n",
        "    L = len(layerDimensions)            \n",
        "    for l in range(1, L-1):\n",
        "        if(stack==False):\n",
        "            parameters['W' + str(l)] = np.random.randn(layerDimensions[l], layerDimensions[l-1])\n",
        "        else:\n",
        "            parameters['W' + str(l)] = para[l-1]['W1']\n",
        "        parameters['b' + str(l)] = np.zeros((layerDimensions[l], 1))\n",
        "    parameters['W' + str(L-1)] = np.random.randn(layerDimensions[L-1], layerDimensions[L-2])\n",
        "    parameters['b' + str(L-1)] = np.zeros((layerDimensions[L-1],1))\n",
        "    return parameters\n",
        "\n",
        "def linearForward(A, W, bias):\n",
        "    Z = np.dot(W, A) + bias\n",
        "    cache = (A, W, bias)\n",
        "    return Z, cache\n",
        "\n",
        "def linearActivationForward(A_prev, W, bias, activation):\n",
        "    \n",
        "    if(activation == \"sigmoid\"):\n",
        "        Z, linear_cache = linearForward(A_prev, W, bias)\n",
        "        A, activation_cache = sigmoidFuntion(Z)\n",
        "    \n",
        "    elif(activation == \"relu\"):\n",
        "        Z, linear_cache = linearForward(A_prev, W, bias)\n",
        "        A, activation_cache = relu(Z)\n",
        "        \n",
        "    cache = (linear_cache, activation_cache)\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def forwardModelL(X, params):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = (len(params) //2)\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linearActivationForward(A, params['W'+str(l)], params['b'+str(l)], \"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    AL, cache = linearActivationForward(A, params['W'+str(L)], params['b'+str(L)], \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL,caches\n",
        "\n",
        "def costComputation(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -(1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "def costComputationAutoencoder(AL, Y, parameters):\n",
        "    m = Y.shape[1]\n",
        "    cost = (1/(2*m))*np.sum(np.linalg.norm(AL-Y))\n",
        "    L = (len(parameters) //2)\n",
        "    \n",
        "    return cost\n",
        "\n",
        "def linearBack(dZ, cache):\n",
        "    A_prev, W, _ = cache\n",
        "    _, m = A_prev.shape\n",
        "\n",
        "    dW = (1/m)*np.dot(dZ, A_prev.T)\n",
        "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
        "    dAP = np.dot(W.T, dZ)\n",
        "   \n",
        "    return dAP, dW, db\n",
        "\n",
        "def linearActivationBack(dA, cache, activation):\n",
        "    linearCache, activationCache = cache\n",
        "    \n",
        "    if (activation == \"relu\"):\n",
        "        dZ = reluBack(dA, activationCache)\n",
        "        dAP, dW, db = linearBack(dZ, linearCache)\n",
        "        \n",
        "    elif (activation == \"sigmoid\"):\n",
        "        dZ = sigmoidBack(dA, activationCache)\n",
        "        dAP, dW, db = linearBack(dZ, linearCache)\n",
        "    \n",
        "    return dAP, dW, db\n",
        "\n",
        "def modelLBack(AL, Y, caches):\n",
        "    gradients = {}\n",
        "    L = len(caches) \n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) \n",
        "    \n",
        "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
        "    \n",
        "    current_cache = caches[L-1]\n",
        "    gradients[\"dA\" + str(L-1)], gradients[\"dW\" + str(L)], gradients[\"db\" + str(L)] = linearActivationBack(dAL, current_cache, \"sigmoid\")\n",
        "   \n",
        "    for l in reversed(range(L-1)):\n",
        "    \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linearActivationBack(gradients[\"dA\"+str(l+1)],current_cache,\"sigmoid\")\n",
        "        gradients[\"dA\" + str(l)] = dA_prev_temp\n",
        "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
        "        gradients[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def updateParams(parameters, gradients, learning_rate):\n",
        "   \n",
        "    L = (len(parameters) //2)\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n",
        "  \n",
        "    return parameters\n",
        "\n",
        "def layerLModel(X, Y, layers_dims, num_iterations,stack, learning_rate = 0.1):\n",
        "\n",
        "    costs = []                        \n",
        " \n",
        "    parameters = initializeParams(layers_dims,para,stack)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        AL, caches = forwardModelL(X,parameters)\n",
        "        \n",
        "        if(stack==True):\n",
        "            cost = costComputation(AL, Y)\n",
        "        else:\n",
        "            cost = costComputationAutoencoder(AL, Y, parameters)\n",
        "        \n",
        "        grads = modelLBack(AL,Y,caches)\n",
        "     \n",
        "        parameters = updateParams(parameters,grads,learning_rate)\n",
        "\n",
        "        if(i%100==0):\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        costs.append(cost)\n",
        "    print(cost)\n",
        "                \n",
        "    return parameters, costs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvuqmA_7YZIl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3NzK3GhC4wg",
        "outputId": "d850a6c4-7891-4661-f126-0f5105f1bb3d"
      },
      "source": [
        "dataset = pd.read_excel('data5.xslx', header = None)\n",
        "\n",
        "row, col = dataset.shape\n",
        "feats = col - 1 \n",
        "\n",
        "# normalization\n",
        "dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "\n",
        "# spliting dataset into train test and val\n",
        "training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "training_data = np.array(training_data)\n",
        "validation_data = np.array(validation_data)\n",
        "testing_data = np.array(testing_data)\n",
        "training_data_X = training_data[:, :feats]\n",
        "training_data_y = training_data[:, feats]\n",
        "validation_data_X = validation_data[:, :feats]\n",
        "validation_data_y = validation_data[:, feats]\n",
        "testing_data_X = testing_data[:, :feats]\n",
        "testing_data_y = testing_data[:, feats]\n",
        "\n",
        "train_row, train_col = training_data_X.shape\n",
        "\n",
        "\n",
        "para = []\n",
        "lr= 0.005\n",
        "\n",
        "layerDemensions = [72,64,72]\n",
        "params, costs = layerLModel(training_data_X, training_data_y, layerDemensions, num_iterations=1000, stack = False, learning_rate=lr)\n",
        "paramsAE1 = params\n",
        "\n",
        "W1=paramsAE1['W1']\n",
        "b1=paramsAE1['b1']\n",
        "X_new,_=linearActivationForward(X.T,W1,b1,\"sigmoid\")\n",
        "X_new.shape\n",
        "\n",
        "\n",
        "layerDemensions = [64,16,64]\n",
        "params, costs = layerLModel(training_data_X, training_data_y, layerDemensions,num_iterations=1000,stack = False,learning_rate=lr)\n",
        "paramsAE2 = params\n",
        "\n",
        "W1 = paramsAE2['W1']\n",
        "b1 = paramsAE2['b1']\n",
        "X_new, _ =  linearActivationForward(X.T, W1, b1, \"sigmoid\")\n",
        "\n",
        "\n",
        "layerDemensions = [16,4,16]\n",
        "params, costs = layerLModel(training_data_X, training_data_y, layerDemensions, learning_rate=lr, num_iterations=1000, stack=False)\n",
        "paramsAE 3= params\n",
        "\n",
        "\n",
        "para=[paramsCopy, paramsAE2, paramsAE3]\n",
        "\n",
        "layerDemensions = [72,64,16,4,1]\n",
        "params, costs = layerLModel(training_data_X, training_data_y, layerDemensions,num_iterations=10000, stack=True, learning_rate=0.125)\n",
        "\n",
        "p, l = forwardModelL(training_data_X, params)\n",
        "p = (p>0.5).astype(int)\n",
        "a = training_data_y-p\n",
        "a = np.sum((a!=0).astype(int))\n",
        "accuracy = (p.shape[1]-a)/p.shape[1]\n",
        "print('Testing Accuracy: ')\n",
        "metrics(p, data_testing_y)\n",
        "print()\n",
        "print('Validation Accuracy: ')\n",
        "metrics(p, data_validation_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: \n",
            "---------------------------------------------------------------------------\n",
            "Sensitivity :  0.8364486046151534\n",
            "Specificity :  0.8787878878788878\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8314356347312943\n",
            "\n",
            "Validation Accuracy: \n",
            "---------------------------------------------------------------------------\n",
            "Sensitivity :  0.8700696134215323\n",
            "Specificity :  0.8689320424531324\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8711183145631243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYXUruq6ZMUd"
      },
      "source": [
        "#Q6\n",
        "\n",
        "Implement extreme learning machine (ELM) classifier for the classification. You can use Gaussian and\n",
        "tanh activation functions. Please select the training and test instances using 5-fold cross-validation\n",
        "technique Evaluate individual accuracy and overall accuracy. The dataset (data5.xlsx) contains 7 features\n",
        "and the last column is the output (class labels). (Packages such as Scikitlearn, keras, tensorflow, pytorch\n",
        "etc. are not allowed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGg3YhgksNMC",
        "outputId": "0e0889eb-626b-4142-c56d-f3b1b5fe096a"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "df = pd.read_excel('data5.xlsx', header = None)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "def sigmoidFunction(Z):\n",
        "    return (1/(1+np.exp(-Z)), Z)\n",
        "\n",
        "para=[]\n",
        "def act(x, a, b, act):\n",
        "    if (act == \"gaussian\"):\n",
        "        return np.exp(-b*np.linalg.norm(x-a))\n",
        "    elif (act == \"tanh\"):\n",
        "        num = 1-np.exp(-(np.dot(x.T,a)+b))\n",
        "        den = 1+np.exp(-(np.dot(x.T,a)+b))\n",
        "        return (num/den)\n",
        "\n",
        "def init(hiddenLayer, dimensions):\n",
        "    a = []\n",
        "    b = []\n",
        "    for i in range(hiddenLayer):\n",
        "        a.append(np.random.rand(dimensions,1))\n",
        "        b.append(np.random.rand(1))\n",
        "    return (a,b)\n",
        "\n",
        "def one_hot(y): \n",
        "    onehotencoder = OneHotEncoder() \n",
        "    y = onehotencoder.fit_transform(y).toarray()\n",
        "    return y\n",
        "\n",
        "def compute(hiddenLayer, train_x, test_x, train_y, test_y, act):\n",
        "\n",
        "    Y_enc = one_hot(train_y)\n",
        "    H = np.zeros((train_x.shape[0],hiddenLayer))\n",
        "    for i in range(H.shape[1]):\n",
        "        for j in range(H.shape[0]):\n",
        "            H[j][i]=act(train_x[j],a[i],b[i],act)\n",
        "    W = np.dot(np.linalg.pinv(H),Y_enc)\n",
        "    \n",
        "    H = np.zeros((test_x.shape[0],hiddenLayer))\n",
        "    for i in range(H.shape[1]):\n",
        "        for j in range(H.shape[0]):\n",
        "            H[j][i] = act(test_x[j],a[i],b[i],act)\n",
        "\n",
        "    p = np.dot(H,W)\n",
        "    p = np.argmax(p,axis=1)\n",
        "    p = np.reshape(p,newshape=(p.shape[0],1))\n",
        "    accuracy = test_y-p\n",
        "    accuracy = np.sum((accuracy!=0).astype(int))\n",
        "    return (p.shape[0]-accuracy)/p.shape[0]\n",
        "\n",
        "l = 256\n",
        "\n",
        "\n",
        "kf = KFold(n_splits = 5)\n",
        "X = df.iloc[:, 0:7].values\n",
        "X = (X - np.mean(X, axis=0))/(np.std(X, axis=0))\n",
        "Y = df.iloc[:,7].values\n",
        "Y = np.reshape(Y, newshape=(-1,1))\n",
        "a, b = init(l, X.shape[1])\n",
        "kf.get_n_splits(X)\n",
        "fold = 0\n",
        "acctemp = 0\n",
        "overall = 0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    fold+=1\n",
        "    training_data_X = X[train_index]\n",
        "    training_data_Y = Y[train_index]\n",
        "    testing_data_X = X[test_index]\n",
        "    testing_data_Y = Y[test_index]\n",
        "    acctemp = compute(l, training_data_X, testing_data_X, training_data_Y, testing_data_Y, act = \"tanh\")\n",
        "    overall+=acctemp\n",
        "    print(\"Fold: \", fold, \"Accuracy: \", acctemp)\n",
        "print(\"Overall Accuracy (tanh) : \", overall/5, '\\n')\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "X=df.iloc[:,0:7].values\n",
        "X=(X-np.mean(X,axis=0))/(np.std(X,axis=0))\n",
        "Y=df.iloc[:,7].values\n",
        "Y=np.reshape(Y,newshape=(-1,1))\n",
        "a,b=init(l,X.shape[1])\n",
        "kf.get_n_splits(X)\n",
        "fold=0\n",
        "accuracy=0\n",
        "overall=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    fold+=1\n",
        "    training_data_X = X[train_index]\n",
        "    training_data_Y = Y[train_index]\n",
        "    testing_data_X = X[test_index]\n",
        "    testing_data_Y = Y[test_index]\n",
        "    accuracy = compute(l,training_data_X,testing_data_X,training_data_Y,testing_data_Y,\"gaussian\")\n",
        "    overall+= accuracy\n",
        "    print(\"Fold: \",fold,\" Accuracy: \", accuracy)\n",
        "print(\"Overall Accuracy (Gaussian) : \" , overall/5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold: 1 Accuracy: 0.913953488372093\n",
            "Fold: 2 Accuracy: 0.8930232558139535\n",
            "Fold: 3 Accuracy: 0.8853146853146853\n",
            "Fold: 4 Accuracy: 0.8930232558139535\n",
            "Fold: 5 Accuracy: 0.8202797202797203\n",
            "Overall Accuracy (tanh) : 0.8011188811188812 \n",
            "\n",
            "Fold: 1 Accuracy: 0.937213488372093\n",
            "Fold: 2 Accuracy: 0.9122232558139535\n",
            "Fold: 3 Accuracy: 0.9255146853146853\n",
            "Fold: 4 Accuracy: 0.8648d32558139535\n",
            "Fold: 5 Accuracy: 0.9234497202797203\n",
            "Overall Accuracy (Gaussian) : 0.9067688811188812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stKMaLbrdHGv"
      },
      "source": [
        "#Q7\n",
        "Implement a deep neural network, which contains two hidden layers (the hidden layers are obtained from\n",
        "the ELM-autoencoders). The last layer will be the ELM layer which means the second hidden layer feature\n",
        "vector is used as input to the ELM classifier. The network can be called as deep layer stacked autoencoder\n",
        "based extreme learning machine. You can use holdout approach (70, 10, 20%) for evaluating the\n",
        "performance of the classifier. The dataset (data5.xlsx) contains 7 features and the last column is the\n",
        "output (class labels). Evaluate individual accuracy and overall accuracy. (Packages such as Scikitlearn,\n",
        "keras, tensorflow, pytorch etc. are not allowed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr76gEC5Mt-C",
        "outputId": "11a24763-b17f-425c-dbef-af9dd1640ad4"
      },
      "source": [
        "def sigmoidFunction(Z):\n",
        "    return 1/(1+np.exp(-Z)),Z\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    return Z,A\n",
        "\n",
        "def reluBackward(dA, cache):\n",
        "    Z = cache\n",
        "    \n",
        "    dZ = np.array(dA, copy=True) \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoidBackward(dA, cache):\n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "def initializeNewLayer(layer_dims,para,stack):\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            \n",
        "    for l in range(1, L-1):\n",
        "        if stack==False:\n",
        "            parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])\n",
        "        else:\n",
        "            parameters['W' + str(l)]=para[l-1]['W1']\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "    parameters['W' + str(L-1)] = np.random.randn(layer_dims[L-1],layer_dims[L-2])\n",
        "    parameters['b' + str(L-1)] = np.zeros((layer_dims[L-1],1))\n",
        "    return parameters\n",
        "\n",
        "def linearForward(A, W, b):\n",
        "    Z = np.dot(W,A)+b\n",
        "    cache = (A, W, b)\n",
        "    return Z,cache\n",
        "\n",
        "def linearActivationFunction(A_prev, W, b, activation):\n",
        "    \n",
        "    if(activation == \"sigmoid\"):\n",
        "        Z, linear_cache = linearForward(A_prev,W,b)\n",
        "        A, activation_cache = sigmoidFunction(Z)\n",
        "    \n",
        "    elif(activation == \"relu\"):\n",
        "        Z, linear_cache = linearForward(A_prev,W,b)\n",
        "        A, activation_cache = relu(Z)\n",
        "        \n",
        "    cache = (linear_cache, activation_cache)\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def modelLForward(X, params):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = (len(params) //2)\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linearActivationFunction(A, params['W'+str(l)], params['b'+str(l)],\"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    AL, cache = linearActivationFunction(A, params['W'+str(L)], params['b'+str(L)],\"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL,caches\n",
        "\n",
        "def costComputation(AL,Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -(1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "def costComputationAE(AL,Y,parameters):\n",
        "    m=Y.shape[1]\n",
        "    cost=(1/(2*m))*np.sum(np.linalg.norm(AL-Y))\n",
        "    L = len(parameters) //2\n",
        "    return cost\n",
        "\n",
        "def linearBack(dZ, cache):\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
        "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "   \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linearActivation(dA, cache, activation):\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = reluBackward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linearBack(dZ,linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoidBackward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linearBack(dZ,linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def backModelL(AL, Y, caches):\n",
        "    grads = {}\n",
        "    L = len(caches) \n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) \n",
        "    \n",
        "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
        "    \n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linearActivation(dAL,current_cache,\"sigmoid\")\n",
        "   \n",
        "    for l in reversed(range(L-1)):\n",
        "    \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linearActivation(grads[\"dA\"+str(l+1)],current_cache,\"sigmoid\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "\n",
        "def updateParams(params, grads, learning_rate):\n",
        "   \n",
        "    L = len(params) //2\n",
        "\n",
        "    for l in range(L):\n",
        "        params[\"W\" + str(l+1)] = params[\"W\" + str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        params[\"b\" + str(l+1)] = params[\"b\" + str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n",
        "  \n",
        "    return params\n",
        "\n",
        "def layerLModel(X, Y, layers_dims, num_iterations,stack, learning_rate = 0.025):\n",
        "\n",
        "    costs = []                        \n",
        " \n",
        "    parameters = initializeNewLayer(layers_dims,para,stack)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        AL, caches = modelLForward(X,parameters)\n",
        "        \n",
        "        if stack==True:\n",
        "            cost = costComputation(AL,Y)\n",
        "        else:\n",
        "            cost = costComputationAE(AL,Y,parameters)\n",
        "        \n",
        "        grads = backModelL(AL,Y,caches)\n",
        "     \n",
        "        parameters = updateParams(parameters,grads,learning_rate)            \n",
        "    \n",
        "    return (parameters, costs)\n",
        "\n",
        "para = []\n",
        "def ACT(x, a, b, act):\n",
        "    if(act == \"gaussian\"):\n",
        "        return np.exp(-b*np.linalg.norm(x-a))\n",
        "    elif(act == \"tanh\"):\n",
        "        num = 1 - np.exp(-(np.dot(x.T, a) + b))\n",
        "        den = 1 + np.exp(-(np.dot(x.T, a) + b))\n",
        "        return (num/den)\n",
        "\n",
        "def init(l_hidden, dimensions):\n",
        "    a = []\n",
        "    b = []\n",
        "    for i in range(l_hidden):\n",
        "        a.append(np.random.rand(dimensions,1))\n",
        "        b.append(np.random.rand(1))\n",
        "    return (a,b)\n",
        "\n",
        "def oneHotEnc(y):\n",
        "    from sklearn.preprocessing import OneHotEncoder \n",
        "    onehotencoder = OneHotEncoder() \n",
        "    y = onehotencoder.fit_transform(y).toarray()\n",
        "    return y\n",
        "\n",
        "def compute(l_hidden,training_data_X,testing_data_X,training_data_y,testing_data_y,act):\n",
        "\n",
        "    Y_enc = oneHotEnc(training_data_y)\n",
        "    H = np.zeros((training_data_X.shape[0],l_hidden))\n",
        "    for i in range(H.shape[1]):\n",
        "        for j in range(H.shape[0]):\n",
        "            H[j][i]=ACT(train_x[j],a[i],b[i],act)\n",
        "    W=np.dot(np.linalg.pinv(H),Y_enc)\n",
        "    \n",
        "    H=np.zeros((testing_data_X.shape[0],l_hidden))\n",
        "    for i in range(H.shape[1]):\n",
        "        for j in range(H.shape[0]):\n",
        "            H[j][i]=ACT(testing_data_X[j],a[i],b[i],act)\n",
        "\n",
        "    p = np.dot(H,W)\n",
        "    p = np.argmax(p,axis=1)\n",
        "    p = np.reshape(p,newshape=(p.shape[0],1))\n",
        "    metrics(p, testing_data_y)\n",
        "\n",
        "\n",
        "dataset = pd.read_excel('data5.xslx', header = None)\n",
        "\n",
        "row, col = dataset.shape\n",
        "feats = col - 1 \n",
        "\n",
        "# normalization\n",
        "dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "\n",
        "# spliting dataset into train test and val\n",
        "training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "training_data = np.array(training_data)\n",
        "validation_data = np.array(validation_data)\n",
        "testing_data = np.array(testing_data)\n",
        "training_data_X = training_data[:, :feats]\n",
        "training_data_y = training_data[:, feats]\n",
        "validation_data_X = validation_data[:, :feats]\n",
        "validation_data_y = validation_data[:, feats]\n",
        "testing_data_X = testing_data[:, :feats]\n",
        "testing_data_y = testing_data[:, feats]\n",
        "\n",
        "train_row, train_col = training_data_X.shape\n",
        "\n",
        "layers_dims = [72,32,72]\n",
        "parameters, costs = layerLModel(training_data_X, training_data_y, layers_dims, num_iterations=1500, stack=False, learning_rate=0.005)\n",
        "paramsAE1 = parameters\n",
        "\n",
        "W1 = paramsAE1['W1']\n",
        "b1 = paramsAE1['b1']\n",
        "X_new, _ = linearActivationFunction(X.T,W1,b1,\"sigmoid\")\n",
        "\n",
        "layers_dims = [32,16,32]\n",
        "parameters,costs = layerLModel(training_data_X, training_data_y, layers_dims,num_iterations=1500,stack=False,learning_rate=0.05, print_cost = True)\n",
        "paramsAE2 = parameters\n",
        "\n",
        "W1 = paramsAE1['W1']\n",
        "W2 = paramsAE2['W1']\n",
        "b1 = paramsAE1['b1']\n",
        "b2 = paramsAE2['b1']\n",
        "\n",
        "x, _ = sigmoidFunction(np.dot(W1,X.T) + b1)\n",
        "x, _ = sigmoidFunction(np.dot(W2,x) + b2)\n",
        "\n",
        "a, b = init(256, X.shape[1])\n",
        "print(\"Tanh Accuracy: \")\n",
        "compute(256, training_data_X, testing_data_X, training_data_y, testing_data_y, \"tanh\")\n",
        "print()\n",
        "a, b = init(256,X.shape[1])\n",
        "print(\"Gaussian Accuracy: \")\n",
        "compute(256,training_data_X,testing_data_X,training_data_y,testing_data_y,\"gaussian\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tanh Accuracy: \n",
            "---------------------------------------------------------------------------\n",
            "Sensitivity :  0.8181818181818182\n",
            "Specificity :  0.8888888888888888\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.855072463768116\n",
            "\n",
            "Gaussian Accuracy: \n",
            "---------------------------------------------------------------------------\n",
            "Sensitivity :  0.9230769230769231\n",
            "Specificity :  0.851063829787234\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8837209302325582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdAnBAJhdIoV"
      },
      "source": [
        "#Q8\n",
        "Implement support vector machine (SVM) classifier for the multi-class classification task. You can use one\n",
        "vs one and one vs all multiclass coding methods to create binary SVM models. Implement the SMO\n",
        "algorithm for the evaluation of the training parameters of SVM such as Lagrange multipliers. You can use\n",
        "holdout approach (70%, 10%, 20%) for evaluating the performance of the classifier. The dataset\n",
        "(data5.xlsx) contains 7 features and the last column is the output (class labels). Evaluate individual\n",
        "accuracy and overall accuracy. You can use RBF and polynomial kernels. Evaluate the classification\n",
        "performance of multiclass SVM for each kernel function. (Packages such as Scikitlearn, keras, tensorflow,\n",
        "pytorch etc. are not allowed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tcv022cdKtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ff95e9-c8dc-40ae-fd9a-b105e6f707f3"
      },
      "source": [
        "\n",
        "dataset = pd.read_excel('data5.xslx', header = None)\n",
        "\n",
        "row, col = dataset.shape\n",
        "feats = col - 1 \n",
        "\n",
        "# normalization\n",
        "dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "\n",
        "# spliting dataset into train test and val\n",
        "training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "training_data = np.array(training_data)\n",
        "validation_data = np.array(validation_data)\n",
        "testing_data = np.array(testing_data)\n",
        "training_data_X = training_data[:, :feats]\n",
        "training_data_y = training_data[:, feats]\n",
        "validation_data_X = validation_data[:, :feats]\n",
        "validation_data_y = validation_data[:, feats]\n",
        "testing_data_X = testing_data[:, :feats]\n",
        "testing_data_y = testing_data[:, feats]\n",
        "\n",
        "train_row, train_col = training_data_X.shape\n",
        "\n",
        "\n",
        "class SupportVec():\n",
        "    def __init__(self, max_iter=10000, kernel_type='linear', C=1.0, epsilon=0.001):\n",
        "        self.kernels = {\n",
        "            'linear' : self.linearKernel\n",
        "        }\n",
        "        self.max_iter = max_iter\n",
        "        self.kernel_type = kernel_type\n",
        "        self.C = C\n",
        "        self.epsilon = epsilon\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        n, d = X.shape[0], X.shape[1]\n",
        "        alpha = np.zeros((n))\n",
        "        kernel = self.kernels[self.kernel_type]\n",
        "        count = 0\n",
        "        while(True):\n",
        "            count += 1\n",
        "            alpha_prev = np.copy(alpha)\n",
        "\n",
        "            for j in range(0, n):\n",
        "                i = self.initRandomize(0, n-1, j) # Get random int i~=j\n",
        "                x_i, x_j, Yi, Yj = X[i,:], X[j,:], y[i], y[j]\n",
        "                k_ij = kernel(x_i, x_i) + kernel(x_j, x_j) - 2 * kernel(x_i, x_j)\n",
        "                if k_ij == 0:\n",
        "                    continue\n",
        "                jPrimeAlpha, iPrimeAlpha = alpha[j], alpha[i]\n",
        "                (L, H) = self.computerLH(self.C, jPrimeAlpha, iPrimeAlpha, Yj, Yi)\n",
        "                self.w = self.computeWeights(alpha, y, X)\n",
        "                self.b = self.computeBias(X, y, self.w)   \n",
        "                E_i = self.E(x_i, Yi, self.w, self.b)\n",
        "                E_j = self.E(x_j, Yj, self.w, self.b)\n",
        "\n",
        "                alpha[j] = jPrimeAlpha + float(Yj * (E_i - E_j))/k_ij\n",
        "                alpha[j] = max(alpha[j], L)\n",
        "                alpha[j] = min(alpha[j], H)\n",
        "\n",
        "                alpha[i] = alpha_prime_i + y_i*Yj * (alpha_prime_j - alpha[j])\n",
        "\n",
        "            diff = np.linalg.norm(alpha - alpha_prev)\n",
        "            if diff < self.epsilon:\n",
        "                break\n",
        "            if(count >= self.max_iter):\n",
        "                print(\"Iteration number exceeded the max of %d iterations\" % (self.max_iter))\n",
        "                return\n",
        "\n",
        "        self.b = self.computeBias(X, y, self.w)\n",
        "        if self.kernel_type == 'linear':\n",
        "            self.w = self.computeWeights(alpha, y, X)\n",
        "\n",
        "        alpha_idx = np.where(alpha > 0)[0]\n",
        "        support_vectors = X[alpha_idx, :]\n",
        "        return support_vectors, count\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.h(X, self.w, self.b)\n",
        "\n",
        "    def computeBias(self, X, y, w):\n",
        "        biasVar = y - np.dot(w.T, X.T)\n",
        "        return np.mean(biasVar)\n",
        "\n",
        "    def computeWeights(self, alpha, Y, X):\n",
        "        return np.dot(X.T, np.multiplY(alpha,y))\n",
        "\n",
        "    def h(self, X, weight, bias):\n",
        "        return np.sign(np.dot(weight.T, X.T) + bias).astype(int)\n",
        "\n",
        "    def E(self, Xk, Yk, weight, bias):\n",
        "        return self.h(Xk, weight, bias) - Yk\n",
        "\n",
        "    def computerLH(self, C, jPrimeAlpha, iPrimeAlpha, Yj, Yi):\n",
        "        if(Yi != Yj):\n",
        "            return (max (0, jPrimeAlpha - iPrimeAlpha), min(C, C - iPrimeAlpha + jPrimeAlpha))\n",
        "        else:\n",
        "            return (max (0, iPrimeAlpha + jPrimeAlpha - C), min(C, iPrimeAlpha + jPrimeAlpha))\n",
        "\n",
        "    def initRandomize(self, a, b, count):\n",
        "\n",
        "        iterations =  count\n",
        "\n",
        "        counter  = 0\n",
        "\n",
        "        while(iterations ==  count and counter<1000):\n",
        "            iterations = random.randint(a,b)\n",
        "            counter += 1\n",
        "        return iterations\n",
        "   \n",
        "    def linearKernel(self, x1, x2):\n",
        "        return np.dot(x1, x2.T)\n",
        "\n",
        "model = SupportVec(max_iter=1000, epsilon=0.01)\n",
        "model.fit(training_data_X, training_data_y)\n",
        "test_pred = model.predict(test_x)\n",
        "print('Testing data')\n",
        "metrics(testing_data_y, test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing data\n",
            "---------------------------------------------------------------------------\n",
            "Sensitivity :  0.8653846153846154\n",
            "Specificity :  0.8043478260869565\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8367346938775511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sgmaH80k8Ck"
      },
      "source": [
        "#Q9\n",
        "Implement a multi-channel 1D deep CNN architecture (as shown in Fig. 1) for the seven-class classification\n",
        "task. The input and the class labels are given in. mat file format. There is a total of 17160 number of\n",
        "instances present in both input and class-label data files. The input data for each instance is a\n",
        "multichannel time series (12-channel) with size as (12 800). The class label for each multichannel time\n",
        "series instance is given in the class_label.mat file. You can select the training and test instances using\n",
        "hold-out cross-validation (70% training, 10% validation, and 20% testing). The architecture of the multichannel deep CNN is given as follows. The number of filters, length of each filter, and number of neurons\n",
        "in the fully connected layers are shown in the following figure. Evaluate individual accuracy and overall\n",
        "accuracy. (Packages such as Scikitlearn, keras, tensorflow, pytorch etc. are allowed)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE9ybNtAlBCF",
        "outputId": "ee1d7c71-d9e2-48fe-c3c6-a6bb70890629"
      },
      "source": [
        "!pip install mat4py\n",
        "import keras \n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Flatten, Conv1D, Dropout,MaxPooling1D,MaxPool1D\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import shuffle\n",
        "from mat4py import loadmat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mat4py in /usr/local/lib/python3.7/dist-packages (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MAKNQt5fzma"
      },
      "source": [
        "data_feats = sio.loadmat('input.mat')\n",
        "data_feats = pd.DataFrame(data_feats[\"x\"])\n",
        "data_feats = (np.asarray(data_feats)).T\n",
        "\n",
        "data_labels=sio.loadmat('class_label.mat') \n",
        "data_labels=np.asarray(data_labels[\"y\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c7OXeJmiU7a"
      },
      "source": [
        "X_feats = []\n",
        "for i in range(len(data_feats)):\n",
        "  X_feats.append(data_feats[i][0])\n",
        "X_feats=np.asarray(X_feats)\n",
        "X_feats=X_feats.transpose(0,2,1)\n",
        "for i in range(len(X_feats)):\n",
        "  X_feats[i]=preprocessing.normalize(X_feats[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcqcBmd_iWuw"
      },
      "source": [
        "Y_labels = []\n",
        "for i in range(len(data_labels)):\n",
        "  Y_labels.append(data_labels[i][0]-1)\n",
        "Y_labels=np.asarray(Y_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSy-9Y9liaOV"
      },
      "source": [
        "training_data_X, testing_data_X, training_data_Y, testing_data_Y = train_test_split(X_feats, Y_labels, test_size = 2/10, train_size = 8/10, random_state = 0)\n",
        "training_data_X, validation_data_X, training_data_Y, validation_data_Y = train_test_split(training_data_X, training_data_Y, test_size=1/8, train_size=7/8, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0zKD544iamf"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(kernel_size=7, filters=20, activation='relu', input_shape=(800,12)))\n",
        "model.add(MaxPooling1D(pool_size = 3, strides = 3))\n",
        "model.add(Conv1D(kernel_size = 7, filters = 60, activation = 'relu'))\n",
        "model.add(MaxPooling1D(pool_size = 3, strides = 3))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Conv1D(filters = 120, kernel_size = 7))\n",
        "model.add(Conv1D(filters = 120, kernel_size = 7))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2000, activation = 'relu'))\n",
        "model.add(Dense(700))\n",
        "model.add(Dense(50))\n",
        "model.add(Dense(7, activation = 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCyLtHuEicwP",
        "outputId": "c4686447-e163-4f1a-9ab6-a9b7f5607010"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 794, 20)           1700      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 264, 20)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 258, 60)           8460      \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 86, 60)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 86, 60)            0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 80, 120)           50520     \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 74, 120)           100920    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8880)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2000)              17762000  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 700)               1400700   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50)                35050     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 7)                 357       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,359,707\n",
            "Trainable params: 19,359,707\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V00F9NMNid8g",
        "outputId": "738fddd0-4e60-402b-a673-25c23275ccc8"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history=model.fit(training_data_X, training_data_Y, epochs=10, batch_size=1000, validation_data = (validation_data_X, validation_data_Y))\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 56s 4s/step - loss: 3.3170 - accuracy: 0.2605 - val_loss: 1.5590 - val_accuracy: 0.4266\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 54s 4s/step - loss: 1.1386 - accuracy: 0.6311 - val_loss: 0.5840 - val_accuracy: 0.7686\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 54s 4s/step - loss: 0.3544 - accuracy: 0.8902 - val_loss: 0.0763 - val_accuracy: 0.9808\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 54s 4s/step - loss: 0.1103 - accuracy: 0.9632 - val_loss: 0.0387 - val_accuracy: 0.9854\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 55s 4s/step - loss: 0.0723 - accuracy: 0.9756 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 54s 4s/step - loss: 0.0146 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 54s 4s/step - loss: 0.0055 - accuracy: 0.9992 - val_loss: 3.0739e-04 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 58s 4s/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 1.9825e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 55s 4s/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 1.6248e-04 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 55s 4s/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 9.4339e-05 - val_accuracy: 1.0000\n",
            "<keras.callbacks.History object at 0x7fd2b47a9150>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNfpcmL6yH3N"
      },
      "source": [
        "#Q10 \n",
        "Implement the hybrid fuzzy deep neural network (HFDNN) for the three-class classification task. The input\n",
        "and output instances for the HFDNN are given in data5.xlsx file (first seven columns input and last column\n",
        "is the output). For a single instance, the input size is 7. There is a total of 210 instances given in the input\n",
        "and label datasets. You can select the training and test instances using hold-out cross-validation\n",
        "(70%training, 10% validation, and 20% testing). The HFDNN architecture shown in Fig. 2 has neural\n",
        "network hidden layers, fuzzy membership and rule layers, and a fusion layer. The descriptions of the HFDNN architecture are given in reference. Evaluate individual accuracy and overall accuracy. (Packages\n",
        "such as Scikitlearn, keras, tensorflow, pytorch etc. are not allowed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGIVplJzjS2R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSMrafV4dVJX"
      },
      "source": [
        "## Downloading as pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "xzX9akOMdW-U",
        "outputId": "84685cb9-9ab4-4eff-e92b-a95ed5bd7554"
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
        "from colab_pdf import colab_pdf\n",
        "colab_pdf('NNFL_A2.ipynb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-01 14:45:55--  https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1864 (1.8K) [text/plain]\n",
            "Saving to: colab_pdf.py\n",
            "\n",
            "colab_pdf.py        100%[===================>]   1.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-01 14:45:56 (5.88 MB/s) - colab_pdf.py saved [1864/1864]\n",
            "\n",
            "Mounted at /content/drive/\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Extracting templates from packages: 100%\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/NNFL_A2.ipynb to pdf\n",
            "[NbConvertApp] Writing 176274 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 119052 bytes to /content/drive/My Drive/NNFL_A2.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f8479837-ab64-4390-9711-17e82f622af7\", \"NNFL_A2.pdf\", 119052)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File ready to be Downloaded and Saved to Drive'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}