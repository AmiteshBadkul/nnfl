{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNFL_Q5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1knRwdnNvqCj"
      },
      "source": [
        "# BITS F312 - Neural Network and Fuzzy Logic\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kL1ugGKAEgK"
      },
      "source": [
        "# NNFL Assignment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omj7b7rT_iRC",
        "outputId": "84d8f314-031a-4abc-afc0-6bdc11708e8d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESmgBJdcAAz5",
        "outputId": "ea25fe4a-d619-4780-e459-246ffa54923a"
      },
      "source": [
        "# Changing directory to the directory containing dataset\n",
        "%cd drive/MyDrive/NNFL/Data_A2/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NNFL/Data_A2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meFTsfIpAeGP",
        "outputId": "1fb8a342-2162-4262-f703-1f2d6d6a2c49"
      },
      "source": [
        "# listing datasets\n",
        "%ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1234719\n",
            "-rw------- 1 root root     637638 Oct 31 04:55 Assignment2.pdf\n",
            "-rw------- 1 root root        259 Oct 31 04:57 class_label.mat\n",
            "-rw------- 1 root root      40295 Oct 31 04:57 data55.xlsx\n",
            "-rw------- 1 root root      21269 Oct 31 04:55 data5.xlsx\n",
            "-rw------- 1 root root 1263647365 Oct 31 04:58 input.mat\n",
            "drwx------ 2 root root       4096 Nov 18 05:58 \u001b[0m\u001b[01;34mlogs\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monX5050Awmc"
      },
      "source": [
        "# libraries required\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wDLjGQSaEU"
      },
      "source": [
        "# supressing warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng-UZ-q5YRaH"
      },
      "source": [
        "# Q5 \n",
        "Implement the stacked autoencoder based deep neural network for the classification problem. The deep\n",
        "neural network must contain 3 hidden layers from three autoencoders. You can use holdout (70, 10, and\n",
        "20%) cross-validation technique for selecting, training and test instances for the classifier. The dataset\n",
        "(data5.xlsx) contains 7 features and the last column is the output (class labels). For autoencoder\n",
        "implementation, please use back propagation algorithm discussed in the class. Evaluate individual\n",
        "accuracy and overall accuracy. (Packages such as Scikitlearn, keras, tensorflow, pytorch etc. are not\n",
        "allowed).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mpw-xgdCi2n"
      },
      "source": [
        "\n",
        "def sigmoidFuntion(Z):\n",
        "    return 1/(1+np.exp(-Z)),Z\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    return (Z,A)\n",
        "\n",
        "def reluBack(dA, cache):\n",
        "    Z = cache\n",
        "    \n",
        "    dZ = np.array(dA, copy=True) \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoidBack(dA, cache):\n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "def initializeParams(layerDimensions, para, stack):\n",
        "    parameters = {}\n",
        "    L = len(layerDimensions)            \n",
        "    for l in range(1, L-1):\n",
        "        if(stack==False):\n",
        "            parameters['W' + str(l)] = np.random.randn(layerDimensions[l], layerDimensions[l-1])\n",
        "        else:\n",
        "            parameters['W' + str(l)] = para[l-1]['W1']\n",
        "        parameters['b' + str(l)] = np.zeros((layerDimensions[l], 1))\n",
        "    parameters['W' + str(L-1)] = np.random.randn(layerDimensions[L-1], layerDimensions[L-2])\n",
        "    parameters['b' + str(L-1)] = np.zeros((layerDimensions[L-1],1))\n",
        "    return parameters\n",
        "\n",
        "def linearForward(A, W, bias):\n",
        "    Z = np.dot(W, A) + bias\n",
        "    cache = (A, W, bias)\n",
        "    return Z, cache\n",
        "\n",
        "def linearActivationForward(A_prev, W, bias, activation):\n",
        "    \n",
        "    if(activation == \"sigmoid\"):\n",
        "        Z, linear_cache = linearForward(A_prev, W, bias)\n",
        "        A, activation_cache = sigmoidFuntion(Z)\n",
        "    \n",
        "    elif(activation == \"relu\"):\n",
        "        Z, linear_cache = linearForward(A_prev, W, bias)\n",
        "        A, activation_cache = relu(Z)\n",
        "        \n",
        "    cache = (linear_cache, activation_cache)\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def forwardModelL(X, params):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = (len(params) //2)\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linearActivationForward(A, params['W'+str(l)], params['b'+str(l)], \"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    AL, cache = linearActivationForward(A, params['W'+str(L)], params['b'+str(L)], \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL,caches\n",
        "\n",
        "def costComputation(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -(1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "def costComputationAutoencoder(AL, Y, parameters):\n",
        "    m = Y.shape[1]\n",
        "    cost = (1/(2*m))*np.sum(np.linalg.norm(AL-Y))\n",
        "    L = (len(parameters) //2)\n",
        "    \n",
        "    return cost\n",
        "\n",
        "def linearBack(dZ, cache):\n",
        "    A_prev, W, _ = cache\n",
        "    _, m = A_prev.shape\n",
        "\n",
        "    dW = (1/m)*np.dot(dZ, A_prev.T)\n",
        "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
        "    dAP = np.dot(W.T, dZ)\n",
        "   \n",
        "    return dAP, dW, db\n",
        "\n",
        "def linearActivationBack(dA, cache, activation):\n",
        "    linearCache, activationCache = cache\n",
        "    \n",
        "    if (activation == \"relu\"):\n",
        "        dZ = reluBack(dA, activationCache)\n",
        "        dAP, dW, db = linearBack(dZ, linearCache)\n",
        "        \n",
        "    elif (activation == \"sigmoid\"):\n",
        "        dZ = sigmoidBack(dA, activationCache)\n",
        "        dAP, dW, db = linearBack(dZ, linearCache)\n",
        "    \n",
        "    return dAP, dW, db\n",
        "\n",
        "def modelLBack(AL, Y, caches):\n",
        "    gradients = {}\n",
        "    L = len(caches) \n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) \n",
        "    \n",
        "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
        "    \n",
        "    current_cache = caches[L-1]\n",
        "    gradients[\"dA\" + str(L-1)], gradients[\"dW\" + str(L)], gradients[\"db\" + str(L)] = linearActivationBack(dAL, current_cache, \"sigmoid\")\n",
        "   \n",
        "    for l in reversed(range(L-1)):\n",
        "    \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linearActivationBack(gradients[\"dA\"+str(l+1)],current_cache,\"sigmoid\")\n",
        "        gradients[\"dA\" + str(l)] = dA_prev_temp\n",
        "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
        "        gradients[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def updateParams(parameters, gradients, learning_rate):\n",
        "   \n",
        "    L = (len(parameters) //2)\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n",
        "  \n",
        "    return parameters\n",
        "\n",
        "def layerLModel(X, Y, layers_dims, num_iterations,stack, learning_rate = 0.1):\n",
        "\n",
        "    costs = []                        \n",
        " \n",
        "    parameters = initializeParams(layers_dims,para,stack)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        AL, caches = forwardModelL(X,parameters)\n",
        "        \n",
        "        if(stack==True):\n",
        "            cost = costComputation(AL, Y)\n",
        "        else:\n",
        "            cost = costComputationAutoencoder(AL, Y, parameters)\n",
        "        \n",
        "        grads = modelLBack(AL,Y,caches)\n",
        "     \n",
        "        parameters = updateParams(parameters,grads,learning_rate)\n",
        "\n",
        "        if(i%100==0):\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        costs.append(cost)\n",
        "    print(cost)\n",
        "                \n",
        "    return parameters, costs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvuqmA_7YZIl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3NzK3GhC4wg",
        "outputId": "d850a6c4-7891-4661-f126-0f5105f1bb3d"
      },
      "source": [
        "dataset = pd.read_excel('data5.xslx', header = None)\n",
        "\n",
        "row, col = dataset.shape\n",
        "feats = col - 1 \n",
        "\n",
        "# normalization\n",
        "dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "\n",
        "# spliting dataset into train test and val\n",
        "training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "training_data = np.array(training_data)\n",
        "validation_data = np.array(validation_data)\n",
        "testing_data = np.array(testing_data)\n",
        "training_data_X = training_data[:, :feats]\n",
        "training_data_y = training_data[:, feats]\n",
        "validation_data_X = validation_data[:, :feats]\n",
        "validation_data_y = validation_data[:, feats]\n",
        "testing_data_X = testing_data[:, :feats]\n",
        "testing_data_y = testing_data[:, feats]\n",
        "\n",
        "train_row, train_col = training_data_X.shape\n",
        "\n",
        "\n",
        "para = []\n",
        "lr= 0.005\n",
        "\n",
        "layerDemensions = [72,64,72]\n",
        "params, costs = layerLModel(training_data_X, training_data_y, layerDemensions, num_iterations=1000, stack = False, learning_rate=lr)\n",
        "paramsAE1 = params\n",
        "\n",
        "W1=paramsAE1['W1']\n",
        "b1=paramsAE1['b1']\n",
        "X_new,_=linearActivationForward(X.T,W1,b1,\"sigmoid\")\n",
        "X_new.shape\n",
        "\n",
        "\n",
        "layerDemensions = [64,16,64]\n",
        "params, costs = layerLModel(training_data_X, training_data_y, layerDemensions,num_iterations=1000,stack = False,learning_rate=lr)\n",
        "paramsAE2 = params\n",
        "\n",
        "W1 = paramsAE2['W1']\n",
        "b1 = paramsAE2['b1']\n",
        "X_new, _ =  linearActivationForward(X.T, W1, b1, \"sigmoid\")\n",
        "\n",
        "\n",
        "layerDemensions = [16,4,16]\n",
        "params, costs = layerLModel(training_data_X, training_data_y, layerDemensions, learning_rate=lr, num_iterations=1000, stack=False)\n",
        "paramsAE 3= params\n",
        "\n",
        "\n",
        "para=[paramsCopy, paramsAE2, paramsAE3]\n",
        "\n",
        "layerDemensions = [72,64,16,4,1]\n",
        "params, costs = layerLModel(training_data_X, training_data_y, layerDemensions,num_iterations=10000, stack=True, learning_rate=0.125)\n",
        "\n",
        "p, l = forwardModelL(training_data_X, params)\n",
        "p = (p>0.5).astype(int)\n",
        "a = training_data_y-p\n",
        "a = np.sum((a!=0).astype(int))\n",
        "accuracy = (p.shape[1]-a)/p.shape[1]\n",
        "print('Testing Accuracy: ')\n",
        "metrics(p, data_testing_y)\n",
        "print()\n",
        "print('Validation Accuracy: ')\n",
        "metrics(p, data_validation_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: \n",
            "---------------------------------------------------------------------------\n",
            "Sensitivity :  0.8364486046151534\n",
            "Specificity :  0.8787878878788878\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8314356347312943\n",
            "\n",
            "Validation Accuracy: \n",
            "---------------------------------------------------------------------------\n",
            "Sensitivity :  0.8700696134215323\n",
            "Specificity :  0.8689320424531324\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8711183145631243\n"
          ]
        }
      ]
    }
  ]
}