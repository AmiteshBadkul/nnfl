{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNFL_Q3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1knRwdnNvqCj"
      },
      "source": [
        "# BITS F312 - Neural Network and Fuzzy Logic\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kL1ugGKAEgK"
      },
      "source": [
        "# NNFL Assignment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omj7b7rT_iRC",
        "outputId": "84d8f314-031a-4abc-afc0-6bdc11708e8d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESmgBJdcAAz5",
        "outputId": "ea25fe4a-d619-4780-e459-246ffa54923a"
      },
      "source": [
        "# Changing directory to the directory containing dataset\n",
        "%cd drive/MyDrive/NNFL/Data_A2/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NNFL/Data_A2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meFTsfIpAeGP",
        "outputId": "1fb8a342-2162-4262-f703-1f2d6d6a2c49"
      },
      "source": [
        "# listing datasets\n",
        "%ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1234719\n",
            "-rw------- 1 root root     637638 Oct 31 04:55 Assignment2.pdf\n",
            "-rw------- 1 root root        259 Oct 31 04:57 class_label.mat\n",
            "-rw------- 1 root root      40295 Oct 31 04:57 data55.xlsx\n",
            "-rw------- 1 root root      21269 Oct 31 04:55 data5.xlsx\n",
            "-rw------- 1 root root 1263647365 Oct 31 04:58 input.mat\n",
            "drwx------ 2 root root       4096 Nov 18 05:58 \u001b[0m\u001b[01;34mlogs\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monX5050Awmc"
      },
      "source": [
        "# libraries required\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wDLjGQSaEU"
      },
      "source": [
        "# supressing warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_kUeuFNh9LV"
      },
      "source": [
        "# Q3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMBG928JS-Do"
      },
      "source": [
        "The dataset (data5.xlsx) contains 7 features and the last column is the output (class labels). Design a\n",
        "multilayer perceptron based neural network (two hidden layers) for the classification. You can use both\n",
        "holdout (70, 10, and 20%) and 5-fold cross-validation approaches for evaluating the performance of the\n",
        "classifier (individual accuracy and overall accuracy). You can select the number of hidden neurons of each\n",
        "hidden layer and other MLP parameters using grid-search method. (Packages such as Scikitlearn, keras,\n",
        "tensorflow, pytorch etc. are not allowed). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8-qxX3Kh7Vu"
      },
      "source": [
        "def relu(x):\n",
        "  \n",
        "  return np.maximum(x,0)\n",
        "\n",
        "def reluDerivative(x):\n",
        "  \n",
        "    return np.heaviside(x,1)\n",
        "\n",
        "def sigmoid(x):\n",
        "  \n",
        "  return 1/(1+ np.exp(-x))\n",
        "\n",
        "def sigmoidDerivative(x):\n",
        "  \n",
        "    return x * (1 - x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LZRJaXDlJb6"
      },
      "source": [
        "def perceptronTraining(train_X, train_y, neuron_L1 = 32, neuron_L2 = 16, epochs = 5000, alpha = 0.00125):\n",
        "  neuron_in = 147\n",
        "  neuron_out = 1\n",
        "  weight_L1 = np.random.uniform(size=(neuron_in,neuron_L1))\n",
        "  bias_L1 = np.random.uniform(size=(1,neuron_L1))\n",
        "  weight_L2 = np.random.uniform(size=(neuron_L1,neuron_L2))\n",
        "  bias_L2 = np.random.uniform(size=(1,neuron_L2))\n",
        "  final_weight = np.random.uniform(size=(neuron_L2,neuron_out))\n",
        "  final_bias = np.random.uniform(size=(1,neuron_out))\n",
        "  costs_perceptron = []\n",
        "  for i in range(epochs):\n",
        "\n",
        "\n",
        "    hiddenInpL1 = np.dot(train_X,weight_L1)\n",
        "    hiddenInpL1=hiddenInpL1 + bias_L1\n",
        "    hiddenActL1 = relu(hiddenInpL1)\n",
        "    hiddenInpL2 = np.dot(hiddenActL1,weight_L2)\n",
        "    hiddenInpL2 = hiddenInpL2 + bias_L2\n",
        "    hiddenActL2 = relu(hiddenInpL2)\n",
        "    outInpL = np.dot(hiddenActL2,final_weight)\n",
        "    outInpL = outInpL+ final_bias\n",
        "    output = sigmoid(outInpL)\n",
        "\n",
        "    error_track = output-train_y\n",
        "    slopeOutL = sigmoidDerivative(output)\n",
        "    slopeHidL2 = reluDerivative(hiddenActL2)\n",
        "    slopeHidL1 = reluDerivative(hiddenActL1)\n",
        "    output_derivative = error_track * slopeOutL\n",
        "    errorHidL2 = output_derivative.dot(final_weight.T)\n",
        "    d_hidden2 = errorHidL2*slopeHidL2 \n",
        "    errorHidL1 = d_hidden2.dot(weight_L2.T)\n",
        "    d_hidden1 = errorHidL1*slopeHidL1\n",
        "    final_weight = final_weight- hiddenActL2.T.dot(output_derivative) *alpha\n",
        "    final_bias = final_bias- np.sum(output_derivative, axis=0,keepdims=True) *alpha\n",
        "    weight_L2 = weight_L2- hiddenActL1.T.dot(d_hidden2) *alpha\n",
        "    bias_L2 = bias_L2- np.sum(d_hidden2, axis=0,keepdims=True) *alpha\n",
        "    weight_L1 = weight_L1- train_X.T.dot(d_hidden1) *alpha\n",
        "    bias_L1 = bias_L1- np.sum(d_hidden1, axis=0,keepdims=True) *alpha\n",
        "\n",
        "    error_track = np.asarray(error_track)\n",
        "    costs_perceptron.append(np.sum(0.5*(error_track*error_track)))\n",
        "\n",
        "  return weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias, costs_perceptron\n",
        "def pred_eval(X, weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias):\n",
        "\n",
        "    hiddenInpL1 = np.dot(X, weight_L1)\n",
        "    hiddenInpL1 = hiddenInpL1 + bias_L1\n",
        "    hiddenActL1 =  relu(hiddenInpL1)\n",
        "    hiddenInpL2 = np.dot(hiddenActL1,weight_L2)\n",
        "    hiddenInpL2 = hiddenInpL2 + bias_L2\n",
        "    hiddenActL2 = relu(hiddenInpL2)\n",
        "    outInpL = np.dot(hiddenActL2,final_weight)\n",
        "    outInpL = outInpL+ final_bias\n",
        "    output = sigmoid(outInpL)\n",
        "\n",
        "    return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmQNDTaDoYiy"
      },
      "source": [
        "def resultQ3(filename = 'data5.xlsx'):\n",
        "  dataset = pd.read_excel(filename, header = None)\n",
        "\n",
        "  row, col = dataset.shape\n",
        "  feats = col - 1 \n",
        "\n",
        "  # normalization\n",
        "  dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "  \n",
        "  # spliting dataset into train test and val\n",
        "  training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "  training_data = np.array(training_data)\n",
        "  validation_data = np.array(validation_data)\n",
        "  testing_data = np.array(testing_data)\n",
        "  training_data_X = training_data[:, :feats]\n",
        "  training_data_y = training_data[:, feats]\n",
        "  validation_data_X = validation_data[:, :feats]\n",
        "  validation_data_y = validation_data[:, feats]\n",
        "  testing_data_X = testing_data[:, :feats]\n",
        "  testing_data_y = testing_data[:, feats]\n",
        "\n",
        "  train_row, train_col = training_data_X.shape\n",
        "\n",
        "  weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias, costs_perceptron = perceptronTraining(training_data_X, training_data_y)\n",
        "  temp_cost = costs_perceptron[-1]\n",
        "\n",
        "  train_pred = pred_eval(training_data_X , weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias)\n",
        "  train_pred = np.where(train_pred > 0.5, 1,0)\n",
        "  print(\"Training Accuracy: \\n\")\n",
        "  metrics(train_pred, training_data_y)\n",
        "  print('{}'.format('-'*75))\n",
        "\n",
        "  test_pred = pred_eval(testing_data_X , weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias)\n",
        "  test_pred = np.where(test_pred > 0.5, 1,0)\n",
        "  print(\"Testing Accuracy: \\n\")\n",
        "  metrics(test_pred, testing_data_y)\n",
        "  print('{}'.format('-'*75))\n",
        "  \n",
        "  validation_pred = pred_eval(validation_data_X , weight_L1, weight_L2, final_weight, bias_L1, bias_L2, final_bias)\n",
        "  validation_pred = np.where(validation_pred > 0.5, 1,0)\n",
        "  print(\"Validation Accuracy: \\n\")\n",
        "  metrics(validation_pred, validation_data_y)\n",
        "  print('{}'.format('-'*75))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91f-QAKioYiz",
        "outputId": "bfbf0e2d-6341-489b-ea50-27f7856d8aeb"
      },
      "source": [
        "resultQ3()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: \n",
            "\n",
            "Sensitivity :  0.8305084745762712\n",
            "Specificity :  0.873015873015873\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8524590163934426\n",
            "---------------------------------------------------------------------------\n",
            "Testing Accuracy: \n",
            "\n",
            "Sensitivity :  0.8706896551724138\n",
            "Specificity :  0.8166666666666667\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8432203389830508\n",
            "---------------------------------------------------------------------------\n",
            "Validation Accuracy: \n",
            "\n",
            "Sensitivity :  0.9285714285714286\n",
            "Specificity :  0.8333333333333334\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8793103448275862\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}