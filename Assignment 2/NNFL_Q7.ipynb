{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNFL_Q7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1knRwdnNvqCj"
      },
      "source": [
        "# BITS F312 - Neural Network and Fuzzy Logic\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kL1ugGKAEgK"
      },
      "source": [
        "# NNFL Assignment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omj7b7rT_iRC",
        "outputId": "84d8f314-031a-4abc-afc0-6bdc11708e8d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESmgBJdcAAz5",
        "outputId": "ea25fe4a-d619-4780-e459-246ffa54923a"
      },
      "source": [
        "# Changing directory to the directory containing dataset\n",
        "%cd drive/MyDrive/NNFL/Data_A2/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NNFL/Data_A2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meFTsfIpAeGP",
        "outputId": "1fb8a342-2162-4262-f703-1f2d6d6a2c49"
      },
      "source": [
        "# listing datasets\n",
        "%ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1234719\n",
            "-rw------- 1 root root     637638 Oct 31 04:55 Assignment2.pdf\n",
            "-rw------- 1 root root        259 Oct 31 04:57 class_label.mat\n",
            "-rw------- 1 root root      40295 Oct 31 04:57 data55.xlsx\n",
            "-rw------- 1 root root      21269 Oct 31 04:55 data5.xlsx\n",
            "-rw------- 1 root root 1263647365 Oct 31 04:58 input.mat\n",
            "drwx------ 2 root root       4096 Nov 18 05:58 \u001b[0m\u001b[01;34mlogs\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monX5050Awmc"
      },
      "source": [
        "# libraries required\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wDLjGQSaEU"
      },
      "source": [
        "# supressing warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stKMaLbrdHGv"
      },
      "source": [
        "#Q7\n",
        "Implement a deep neural network, which contains two hidden layers (the hidden layers are obtained from\n",
        "the ELM-autoencoders). The last layer will be the ELM layer which means the second hidden layer feature\n",
        "vector is used as input to the ELM classifier. The network can be called as deep layer stacked autoencoder\n",
        "based extreme learning machine. You can use holdout approach (70, 10, 20%) for evaluating the\n",
        "performance of the classifier. The dataset (data5.xlsx) contains 7 features and the last column is the\n",
        "output (class labels). Evaluate individual accuracy and overall accuracy. (Packages such as Scikitlearn,\n",
        "keras, tensorflow, pytorch etc. are not allowed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr76gEC5Mt-C",
        "outputId": "11a24763-b17f-425c-dbef-af9dd1640ad4"
      },
      "source": [
        "def sigmoidFunction(Z):\n",
        "    return 1/(1+np.exp(-Z)),Z\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    return Z,A\n",
        "\n",
        "def reluBackward(dA, cache):\n",
        "    Z = cache\n",
        "    \n",
        "    dZ = np.array(dA, copy=True) \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoidBackward(dA, cache):\n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "def initializeNewLayer(layer_dims,para,stack):\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            \n",
        "    for l in range(1, L-1):\n",
        "        if stack==False:\n",
        "            parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])\n",
        "        else:\n",
        "            parameters['W' + str(l)]=para[l-1]['W1']\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "    parameters['W' + str(L-1)] = np.random.randn(layer_dims[L-1],layer_dims[L-2])\n",
        "    parameters['b' + str(L-1)] = np.zeros((layer_dims[L-1],1))\n",
        "    return parameters\n",
        "\n",
        "def linearForward(A, W, b):\n",
        "    Z = np.dot(W,A)+b\n",
        "    cache = (A, W, b)\n",
        "    return Z,cache\n",
        "\n",
        "def linearActivationFunction(A_prev, W, b, activation):\n",
        "    \n",
        "    if(activation == \"sigmoid\"):\n",
        "        Z, linear_cache = linearForward(A_prev,W,b)\n",
        "        A, activation_cache = sigmoidFunction(Z)\n",
        "    \n",
        "    elif(activation == \"relu\"):\n",
        "        Z, linear_cache = linearForward(A_prev,W,b)\n",
        "        A, activation_cache = relu(Z)\n",
        "        \n",
        "    cache = (linear_cache, activation_cache)\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def modelLForward(X, params):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = (len(params) //2)\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linearActivationFunction(A, params['W'+str(l)], params['b'+str(l)],\"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    AL, cache = linearActivationFunction(A, params['W'+str(L)], params['b'+str(L)],\"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL,caches\n",
        "\n",
        "def costComputation(AL,Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -(1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "def costComputationAE(AL,Y,parameters):\n",
        "    m=Y.shape[1]\n",
        "    cost=(1/(2*m))*np.sum(np.linalg.norm(AL-Y))\n",
        "    L = len(parameters) //2\n",
        "    return cost\n",
        "\n",
        "def linearBack(dZ, cache):\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
        "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "   \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linearActivation(dA, cache, activation):\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = reluBackward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linearBack(dZ,linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoidBackward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linearBack(dZ,linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def backModelL(AL, Y, caches):\n",
        "    grads = {}\n",
        "    L = len(caches) \n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) \n",
        "    \n",
        "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
        "    \n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linearActivation(dAL,current_cache,\"sigmoid\")\n",
        "   \n",
        "    for l in reversed(range(L-1)):\n",
        "    \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linearActivation(grads[\"dA\"+str(l+1)],current_cache,\"sigmoid\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "\n",
        "def updateParams(params, grads, learning_rate):\n",
        "   \n",
        "    L = len(params) //2\n",
        "\n",
        "    for l in range(L):\n",
        "        params[\"W\" + str(l+1)] = params[\"W\" + str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        params[\"b\" + str(l+1)] = params[\"b\" + str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n",
        "  \n",
        "    return params\n",
        "\n",
        "def layerLModel(X, Y, layers_dims, num_iterations,stack, learning_rate = 0.025):\n",
        "\n",
        "    costs = []                        \n",
        " \n",
        "    parameters = initializeNewLayer(layers_dims,para,stack)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        AL, caches = modelLForward(X,parameters)\n",
        "        \n",
        "        if stack==True:\n",
        "            cost = costComputation(AL,Y)\n",
        "        else:\n",
        "            cost = costComputationAE(AL,Y,parameters)\n",
        "        \n",
        "        grads = backModelL(AL,Y,caches)\n",
        "     \n",
        "        parameters = updateParams(parameters,grads,learning_rate)            \n",
        "    \n",
        "    return (parameters, costs)\n",
        "\n",
        "para = []\n",
        "def ACT(x, a, b, act):\n",
        "    if(act == \"gaussian\"):\n",
        "        return np.exp(-b*np.linalg.norm(x-a))\n",
        "    elif(act == \"tanh\"):\n",
        "        num = 1 - np.exp(-(np.dot(x.T, a) + b))\n",
        "        den = 1 + np.exp(-(np.dot(x.T, a) + b))\n",
        "        return (num/den)\n",
        "\n",
        "def init(l_hidden, dimensions):\n",
        "    a = []\n",
        "    b = []\n",
        "    for i in range(l_hidden):\n",
        "        a.append(np.random.rand(dimensions,1))\n",
        "        b.append(np.random.rand(1))\n",
        "    return (a,b)\n",
        "\n",
        "def oneHotEnc(y):\n",
        "    from sklearn.preprocessing import OneHotEncoder \n",
        "    onehotencoder = OneHotEncoder() \n",
        "    y = onehotencoder.fit_transform(y).toarray()\n",
        "    return y\n",
        "\n",
        "def compute(l_hidden,training_data_X,testing_data_X,training_data_y,testing_data_y,act):\n",
        "\n",
        "    Y_enc = oneHotEnc(training_data_y)\n",
        "    H = np.zeros((training_data_X.shape[0],l_hidden))\n",
        "    for i in range(H.shape[1]):\n",
        "        for j in range(H.shape[0]):\n",
        "            H[j][i]=ACT(train_x[j],a[i],b[i],act)\n",
        "    W=np.dot(np.linalg.pinv(H),Y_enc)\n",
        "    \n",
        "    H=np.zeros((testing_data_X.shape[0],l_hidden))\n",
        "    for i in range(H.shape[1]):\n",
        "        for j in range(H.shape[0]):\n",
        "            H[j][i]=ACT(testing_data_X[j],a[i],b[i],act)\n",
        "\n",
        "    p = np.dot(H,W)\n",
        "    p = np.argmax(p,axis=1)\n",
        "    p = np.reshape(p,newshape=(p.shape[0],1))\n",
        "    metrics(p, testing_data_y)\n",
        "\n",
        "\n",
        "dataset = pd.read_excel('data5.xslx', header = None)\n",
        "\n",
        "row, col = dataset.shape\n",
        "feats = col - 1 \n",
        "\n",
        "# normalization\n",
        "dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "\n",
        "# spliting dataset into train test and val\n",
        "training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "training_data = np.array(training_data)\n",
        "validation_data = np.array(validation_data)\n",
        "testing_data = np.array(testing_data)\n",
        "training_data_X = training_data[:, :feats]\n",
        "training_data_y = training_data[:, feats]\n",
        "validation_data_X = validation_data[:, :feats]\n",
        "validation_data_y = validation_data[:, feats]\n",
        "testing_data_X = testing_data[:, :feats]\n",
        "testing_data_y = testing_data[:, feats]\n",
        "\n",
        "train_row, train_col = training_data_X.shape\n",
        "\n",
        "layers_dims = [72,32,72]\n",
        "parameters, costs = layerLModel(training_data_X, training_data_y, layers_dims, num_iterations=1500, stack=False, learning_rate=0.005)\n",
        "paramsAE1 = parameters\n",
        "\n",
        "W1 = paramsAE1['W1']\n",
        "b1 = paramsAE1['b1']\n",
        "X_new, _ = linearActivationFunction(X.T,W1,b1,\"sigmoid\")\n",
        "\n",
        "layers_dims = [32,16,32]\n",
        "parameters,costs = layerLModel(training_data_X, training_data_y, layers_dims,num_iterations=1500,stack=False,learning_rate=0.05, print_cost = True)\n",
        "paramsAE2 = parameters\n",
        "\n",
        "W1 = paramsAE1['W1']\n",
        "W2 = paramsAE2['W1']\n",
        "b1 = paramsAE1['b1']\n",
        "b2 = paramsAE2['b1']\n",
        "\n",
        "x, _ = sigmoidFunction(np.dot(W1,X.T) + b1)\n",
        "x, _ = sigmoidFunction(np.dot(W2,x) + b2)\n",
        "\n",
        "a, b = init(256, X.shape[1])\n",
        "print(\"Tanh Accuracy: \")\n",
        "compute(256, training_data_X, testing_data_X, training_data_y, testing_data_y, \"tanh\")\n",
        "print()\n",
        "a, b = init(256,X.shape[1])\n",
        "print(\"Gaussian Accuracy: \")\n",
        "compute(256,training_data_X,testing_data_X,training_data_y,testing_data_y,\"gaussian\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tanh Accuracy: \n",
            "---------------------------------------------------------------------------\n",
            "Sensitivity :  0.8181818181818182\n",
            "Specificity :  0.8888888888888888\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.855072463768116\n",
            "\n",
            "Gaussian Accuracy: \n",
            "---------------------------------------------------------------------------\n",
            "Sensitivity :  0.9230769230769231\n",
            "Specificity :  0.851063829787234\n",
            "Accuracy ((TN+TP)/(TN+TP+FN+FP)) :  0.8837209302325582\n"
          ]
        }
      ]
    }
  ]
}